"""
ğŸ¤— PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’èª­ã¿è¾¼ã‚“ã§ä½¿ç”¨ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªä¾‹
"""
from transformers import AutoModelForCausalLM, AutoTokenizer

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«IDã¨PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ID
base_model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
print(f"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­: {base_model_id}")
tokenizer = AutoTokenizer.from_pretrained(base_model_id)

# PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
print(f"PEFTã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {peft_model_id}")
model = AutoModelForCausalLM.from_pretrained(peft_model_id)

# ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
text = "Hello, my name is"
inputs = tokenizer(text, return_tensors="pt")

print(f"\nå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
print("ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆä¸­...")

outputs = model.generate(**inputs, max_new_tokens=20)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"\nç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {generated_text}")
